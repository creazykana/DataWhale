## GBDT算法梳理

### 1. 前向分布算法
对于一个加法模型(由M个基函数按系数相加组成)，在给定训练数据集和损失函数的条件下，学习加法模型成为损失函数最小化的问题。  
通常这是一个复杂的优化问题，前向分布算法求解这一优化问题的思想是：因为学习的是加法模型，如果能够从前向后(按照线性关系的顺序)，每一步之学习
一个奇函数及其系数，逐渐逼近优化目标函数式，那么就可以简化优化的复杂度。

### 2. 负梯度拟合
#### 提升树
采用加法模型与前向分步算法，以决策树作为基函数的提升方法称为提升树。提升树模型可以表示为决策树的加法模型。  
回归问题提升树使用前向分步算法，在第m步中，给定当前模型fm-1(x)，求解使得平方误差函数最小的参数进而得到第m棵树。  
其中平方损失函数L=[y-fm-1(x)-T]^2=[r-T]^2.其中r=y-fm-1(x)。  
*个人理解：即在拟合第m棵树的时候，实际上拟合第目标是使第m棵树的预测值与前面m-1棵树的预测值的误差尽可能相近，用第m棵树来减少前面m-1棵树的误差。*
#### 梯度提升
提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失函数和指数损失函数时，每一步优化是很简单的，但对一般的损失函数而言，往往每
一步优化并不那么容易。针对这一问题，Freidman提出了梯度提升算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值作为回归问题
提升树算法中的残差的近似值，拟合一个回归树。

### 3. 损失函数
对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:  
如果是指数损失函数，则损失函数表达式为其负梯度计算和叶子节点的最佳残差拟合  
如果是对数损失函数，分为二元分类和多元分类两种。  
对于回归算法，常用损失函数有如下4种:  
1）均方差  
2）绝对损失  
3）Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。  
4）分位数损失。它对应的是分位数回归的损失函数。  

### 4. 回归

### 5. 二分类，多分类

### 6. 正则化

### 7. 优缺点

### 8. sklearn参数

### 9. 应用场景
