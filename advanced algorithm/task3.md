## Xgboost算法原理

#### 1.算法原理  
xgboost本质上还是一个GBDT，xgboost与gbdt比较大的不同就是目标函数的定义。  
不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差；
当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数；
最后只需要将每棵树对应的分数加起来就是该样本的预测值。  

#### 2.损失函数
损失函数的定义似乎有多种，最终可以通过泰勒展开成多项式进行迭代。  

#### 3.分裂节点算法  
对于一个叶子节点如何进行分裂，xgboost作者在其原始论文中给出了两种分裂节点的方法  
(1)枚举所有不同树结构的贪心法  
不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作(要枚举的状态太多)  
贪心法：从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，先按照该特征里的值进行排序，
然后线性扫描该特征进而确定最好的分割点，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征  

(2)近似算法  
略  

#### 4.正则化
一棵树有T个叶子节点，这T个叶子节点的值组成了一个T维向量w，q(x)是一个映射，用来将样本映射成1到T的某个值，
也就是把它分到某个叶子节点，q(x)其实就代表了CART树的结构。w_q(x)自然就是这棵树对样本x的预测值了  
正则化项：


#### 5.对缺失值处理  
xgboost把缺失值当做稀疏矩阵来对待，本身在节点分裂时不考虑缺失值的数值，但确定分裂的特征后，缺失值数据会被分到左子树和右子树呢？
本文的处理策略是落在哪个孩子得分高，就放到哪里。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树  
来源：https://blog.csdn.net/wangrongrongwq/article/details/86755915#6.%E5%AF%B9%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86

#### 6.优缺点  
速度快精度高  
https://blog.csdn.net/hfzd24/article/details/76889428  

#### 7.应用场景  
评分系统，智能垃圾邮件识别  

#### 8.sklearn参数  
https://blog.csdn.net/wangrongrongwq/article/details/86755915#8.xgb%E5%8F%82%E6%95%B0
